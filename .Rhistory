library(randomForest)
library(dplyr)
library(caret)
library(xgboost)
library(ggplot2)
#
# 7/19/22 made changes to read new manually adjusted tree locations
# changed file name, columns, bestmtry
#
# 10/26/2022...after move
# Still trying to figure out the behavior where I get the same numbers for false predictions using all data
# that I get using the testing data. This only happens if I fit the RF model using the default for maxnodes.
# If I set maxnodes = bestmtry - 1, I get behavior that makes sense.
#
# set this flag variable if you want to use the metrics for the point clips for leaning trees that were
# created using smaller point clips around the tree locations and also accounting for tree lean as
# measured when Ally & Bob adjusted tree locations. Hopefully this set of metrics produces better
# model performance.
useLeaningTrees <- TRUE
# read input file...format varies slighlty between the original file and the leaning tree file
# testing: inputDataFile <- "G:/R_Stuff/ONRCDroneLidar/T3_AdjXY_Training_TreeTops_AllPlots.csv"
# testing: inputDataFile <- "G:/R_Stuff/ONRCDroneLidar/AdjustedField_T3_Training_TreeTops_AllPlots.csv"
if (useLeaningTrees) {
inputDataFile <- "H:/T3_DroneLidar/Leaning_TreeTops_SmallCylinder_normalized_metrics_10_25_2023.csv"
} else {
inputDataFile <- "extras/AdjustedField_T3_Training_TreeTops_AllPlots.csv"
}
# read data
inputData <- read.csv(inputDataFile, stringsAsFactors = FALSE)
library(randomForest)
library(dplyr)
library(caret)
library(xgboost)
library(ggplot2)
#
# 7/19/22 made changes to read new manually adjusted tree locations
# changed file name, columns, bestmtry
#
# 10/26/2022...after move
# Still trying to figure out the behavior where I get the same numbers for false predictions using all data
# that I get using the testing data. This only happens if I fit the RF model using the default for maxnodes.
# If I set maxnodes = bestmtry - 1, I get behavior that makes sense.
#
# set this flag variable if you want to use the metrics for the point clips for leaning trees that were
# created using smaller point clips around the tree locations and also accounting for tree lean as
# measured when Ally & Bob adjusted tree locations. Hopefully this set of metrics produces better
# model performance.
useLeaningTrees <- TRUE
# read input file...format varies slighlty between the original file and the leaning tree file
# testing: inputDataFile <- "G:/R_Stuff/ONRCDroneLidar/T3_AdjXY_Training_TreeTops_AllPlots.csv"
# testing: inputDataFile <- "G:/R_Stuff/ONRCDroneLidar/AdjustedField_T3_Training_TreeTops_AllPlots.csv"
if (useLeaningTrees) {
inputDataFile <- "H:/T3_DroneLidar/Leaning_TreeTops_SmallCylinder_normalized_metrics_10_25_2023.csv"
} else {
inputDataFile <- "extras/AdjustedField_T3_Training_TreeTops_AllPlots.csv"
}
# read data
inputData <- read.csv(inputDataFile, stringsAsFactors = FALSE)
# compute relative percentiles...divide P?? by P99. I use these when dealing with clips of individual trees
# to allow the use of percentiles in cases where one species is much higher than another
#
# for this case with the point clips for the upper 3m of crowns, these are less useful (but still useful)
inputData$RP01 <- inputData$Elev.P01 / inputData$Elev.P99
inputData$RP05 <- inputData$Elev.P05 / inputData$Elev.P99
inputData$RP10 <- inputData$Elev.P10 / inputData$Elev.P99
inputData$RP20 <- inputData$Elev.P20 / inputData$Elev.P99
inputData$RP25 <- inputData$Elev.P25 / inputData$Elev.P99
inputData$RP30 <- inputData$Elev.P30 / inputData$Elev.P99
inputData$RP40 <- inputData$Elev.P40 / inputData$Elev.P99
inputData$RP50 <- inputData$Elev.P50 / inputData$Elev.P99
inputData$RP60 <- inputData$Elev.P60 / inputData$Elev.P99
inputData$RP70 <- inputData$Elev.P70 / inputData$Elev.P99
inputData$RP75 <- inputData$Elev.P75 / inputData$Elev.P99
inputData$RP80 <- inputData$Elev.P80 / inputData$Elev.P99
inputData$RP90 <- inputData$Elev.P90 / inputData$Elev.P99
inputData$RP95 <- inputData$Elev.P95 / inputData$Elev.P99
#colnames(inputData)
# do some analyses related to the amount of lean and the offset from the tree base to the tree top
# useLeaningTrees must be TRUE so we have the lean info
if (useLeaningTrees) {
aveHt <- (inputData$Total.Height.Ally + inputData$Total.Height.Bob) / 2
aveLean <- (inputData$Lean.Angle.From.Vertical.Ally + inputData$Lean.Angle.From.Vertical.Bob) / 2
aveLeanAzimuth <- (inputData$Lean.Azimuth.Ally + inputData$Lean.Azimuth.Bob) / 2
offset <- sin(aveLean * pi / 180.0) * aveHt
cat("Summary of offset distances(m)\n")
print(summary(offset))
cat("Std Dev offset distances: ", sd(offset), "\n")
cat("\nSummary of lean angles (degrees)\n")
print(summary(aveLean))
cat("Std Dev lean angle: ", sd(aveLean), "\n")
hist(aveLean, main = "", xlab = "Tree lean (degrees)", labels = TRUE)
hist(offset, main = "", xlab = "Offset from tree base to tree top (meters)", labels = TRUE)
}
# end of lean analyses
DropWindyPlots <- FALSE
if (DropWindyPlots) {
# drop plots with wind
inputData <- dplyr::filter(inputData, Plot_Number != 18 & Plot_Number != 25 & Plot_Number != 27 & Plot_Number != 29 & Plot_Number != 34)
}
# Sorting out column numbers can be a real pain. Easy way is to load the csv file into excel. Then copy the column labels (entire first row).
# open a new spreadsheet (or tab in the current spreadsheet), put the cursor in row 1 column B and paste special...select transpose. Then
# create sequential numbers in column A (put 1, 2, 3 in first three rows, select them and double click the small rectangle in the lower
# right corner of the selection box).
if (useLeaningTrees) {
# extract useful columns...by number
# 48 species
# 91:128 metrics directly related to height...all clips should have heights from 0-3m so these metrics are OK to use for RF
# 129:161 intensity metrics
# 162 profile area
# 163:176 relative percentiles
# this is all lidar metrics...89.0% accuracy
modelData <- inputData[, c(48, 91:128, 129:161, 162, 163:176)]
} else {
# extract useful columns...by number
# 49 species
# 98:135 metrics directly related to height...all clips should have heights from 0-3m so these metrics are OK to use for RF
# 136:168 intensity metrics
# 169 profile area
# 180:193 relative percentiles
# this is all lidar metrics...89.0% accuracy
modelData <- inputData[, c(49, 98:135, 136:168, 169, 180:193)]
}
if (useLeaningTrees) {
#modelData <- inputData[, c(48, 91:128, 163:176)]
} else {
# this is only height related metrics...no intensity...84.5% accuracy...for original tree metrics
#modelData <- inputData[, c(49, 98:135, 180:193)]
}
# filter out all species except PSME & TSHE...only 31 trees that could be matched to lidar trees
modelData <- dplyr::filter(modelData, Species == "PSME" | Species == "TSHE")
# make species a factor...do this after filtering for PSME & TSHE so the factor values only have the 2 species
# if you make Species a factor before filtering, you get errors in the call to randomForest
# you need to have the response variable for your model as a factor or else randomForest will do regression
modelData$Species <- as.factor(modelData$Species)
allData <- modelData
# write the data used for model development and testing
write.csv(allData, "H:/T3_DroneLidar/Leaning_TreeTops_SmallCylinder_normalized_metrics_ModelTraining.csv")
# *****************************************************************************
# very important!!!
# the row/column arrangement in the confusion matrix output by randomForest
# is the opposite of that output by confusionMatrix
# for the RF output, read across the rows to evaluate the classification
# but for confusionMatrix output, read down the columns to evaluate
# *****************************************************************************
modelData <- allData
if (useLeaningTrees) {
# drop rows with bad metrics
modelData <- modelData[modelData$Elev.stddev > 0, ]
}
# drop P95 and P99...testing RF behavior
#modelData <- modelData[, -c(35,36)]
# limit variables
#modelData <- allData[, c("Species", "Elev.P99", "Elev.P95", "Int.L.skewness", "Int.L3")]
# drop the min/max values for elevation because we expect these to be 0 and 3.0 or very close
modelData <- modelData[, -c(2, 3)]
# drop all intensity variables
dropIntensity <- FALSE
dropElevation <- FALSE
if (dropIntensity) {
modelData <- modelData[, c(1:37, 71:85)]
}
if (dropElevation) {
modelData <- modelData[, c(1, 38:70)]
}
# set seed so we can replicate results
set.seed(15356)
# randomly split data into training and testing sets...the split seems to matter. I did
# testing (see end of code) to evaluate the split proportion and the stability of the
# error rates (cross validation with averaged OOB and accuracy). However, I didn't really
# come up with a good strategy for selecting the best split. Intuitively, you want to
# have a model that fits well (low OOB) and also performs well with "new" data (low error
# with testing data). I looked at CV of OOB and testing-data error as possible criteria for
# selecting a final split. My logic is that you want the split that gives roughly the same
# error for both and that produces low CV (indicating model and prediction stability, I think).
#
# The problem is that when you do the cross validation, you are looking at results over
# multiple iterations but you ultimately have to use a single iteration to predict species
# for "new" data so you really don't know anything about the single-iteration model's
# stability.
#
# I think the final strategy is to fit a model using a reasonable split (informed by
# the sensitivity analysis), report the OOB, Kappa, and error for the testing data. Then
# run cross validation using the same split to assess the stability of the split and
# predictor variables. Report the average OOB, Kappa and testing error to indicate that
# our overall data set is providing a classifier that is robust enough for use.
#
# Alternatively, do the cross validation testing and use the results to determine the
# split. I did this and looked for the split that produced the most similar values for
# OOB and error with testing data. 55/35 split was the "best".
#
# Another question is the data used for tuning. Should you use the training data only
# or all data? The tuneRanger() function uses prediction accuracy as the test criteria
# (measure can be changed) but doesn't use a data split. I think it makes the most sense to
# tune parameters using the split data but I haven't been able to find any guidence online.
#
# I did some testing at the end of the code. A 50/50 or 60/40 split seems to produce
# a model with fairly low OOB error and low error when predicting the testing data
train_indx = sample(x = 1:nrow(modelData),
size = 0.55 * nrow(modelData),
replace = FALSE)
trainingData <- modelData[train_indx,]
testingData <- modelData[-train_indx,]
#ind = sample(2, nrow(modelData), replace=FALSE, prob=c(0.8,0.2))
#trainingData = modelData[ind == 1, ]
#testingData = modelData[ind == 2, ]
table(trainingData$Species)
table(testingData$Species)
table(modelData$Species)
modelData %>%
group_by(Species) %>%
summarize(
Count = n(),
AveP99 = mean(Elev.P99, na.rm = TRUE),
IntP80 = mean(Int.P80, na.rm = TRUE),
IntP20 = mean(Int.P20, na.rm = TRUE),
IntIQ = mean(Int.IQ, na.rm = TRUE)
)
trainingData %>%
group_by(Species) %>%
summarize(
Count = n(),
AveP99 = mean(Elev.P99, na.rm = TRUE),
IntP80 = mean(Int.P80, na.rm = TRUE),
IntP20 = mean(Int.P20, na.rm = TRUE),
IntIQ = mean(Int.IQ, na.rm = TRUE)
)
testingData %>%
group_by(Species) %>%
summarize(
Count = n(),
AveP99 = mean(Elev.P99, na.rm = TRUE),
IntP80 = mean(Int.P80, na.rm = TRUE),
IntP20 = mean(Int.P20, na.rm = TRUE),
IntIQ = mean(Int.IQ, na.rm = TRUE)
)
# flag for model tuning. If TRUE, only mtry is tuned. Otherwise, more parameters are tuned
# using caret procedures from https://stackoverflow.com/questions/57939453/building-a-randomforest-with-caret
#
# *****downside is that the more complex tuning requires that you look at the outputs to find the best set of
# hyperparameters. This would be much better if the code selected the best set and then used the parameters
# to fit the final model.
#
# The more advanced tuning also takes much longer to run. In the initial testing, the improvement in model
# performance wasn't that great.
simpleTune <- FALSE
probstTune <- TRUE
if (simpleTune) {
# use a tuning function to find the best mtry value (lowest OOB error)
# the best value will very depending on the random number seed (be careful if you rerun this code without initializing the seed above)
bestmtry <- tuneRF(trainingData[, -1], trainingData$Species, ntreeTry = 1000, stepFactor = 1.5, improve = 0.01, trace=T, plot= T)
i <- which(bestmtry==min(bestmtry), arr.ind = TRUE)
bestmtry <- bestmtry[i[1]]
#bestmtry <- 25
#bestmtry <- 19
print(paste("Best value for mtry = ", bestmtry))
# build a random forest model using the training data... we have more TSHE than PSME so use sampsize
# parameter to equalize the sample. Balanced sample probably isn't needed...species counts aren't that
# different (247 PSME & 278 TSHE). Accuracy is slightly (0.65%) better when NOT balancing the sample.
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = 1000
#                       , nodesize = 4
, maxnodes = bestmtry - 1
)
} else if (probstTune == FALSE) {
# create predefined folds
set.seed(1234)
cv_folds <- createFolds(modelData$Species, k = 5, returnTrain = TRUE)
# create tune control
tuneGrid <- expand.grid(.mtry = c(1 : 15))
# default summary
# ***** need to define metric = "Kappa" in call to train()
# ctrl <- trainControl(method = "cv",
#                      number = 5,
#                      search = 'grid',
#                      classProbs = TRUE,
#                      savePredictions = "final",
#                      index = cv_folds)
ctrl <- trainControl(method = "cv",
number = 5,
search = 'grid',
classProbs = TRUE,
savePredictions = "final",
index = cv_folds,
summaryFunction = twoClassSummary) #in most cases a better summary for two class problems
# add more parameters to tune...also adds run time
ntrees <- c(250, 500, 1000)
nodesize <- c(1, 2, 4, 6)
params <- expand.grid(ntrees = ntrees,
nodesize = nodesize)
# train model
store_maxnode <- vector("list", nrow(params))
for(i in 1:nrow(params)) {
nodesize <- params[i,2]
ntree <- params[i,1]
set.seed(65)
rf_model <- train(Species~.,
data = modelData,
method = "rf",
importance=TRUE,
metric = "ROC",
#                      metric = "Kappa",
tuneGrid = tuneGrid,
trControl = ctrl,
ntree = ntree,
nodesize = nodesize)
store_maxnode[[i]] <- rf_model
}
# add names to list using parameters
names(store_maxnode) <- paste("ntrees:", params$ntrees,
"nodesize:", params$nodesize)
# combine results
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
# get best mtry for model
lapply(store_maxnode, function(x) x$best)
# get best average performance for models
t <- lapply(store_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])
# find best model using ROC...should be an easier way to access the list of RF models but
# I couldn't figure it out so I looped.
besti <- 0
bestmtry <- 0
bestnodesize <- 0
bestntree <- 0
bestROC <- 0
for(i in 1:nrow(params)) {
if (t[[i]]$ROC > bestROC) {
besti <- i
bestROC <- t[[i]]$ROC
bestmtry <- t[[i]]$mtry
bestnodesize <- params[i,2]
bestntree <- params[i,1]
}
}
# run best model after looking at results to find model with highest ROC
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = bestntree
, nodesize = bestnodesize
)
print(paste("Best model based on area under ROC:"))
print(paste("   mtry = ", bestmtry))
print(paste("   ntree = ", bestntree))
print(paste("   nodesize = ", bestnodesize))
print(paste("   ROC value = ", bestROC))
} else {
# probst tune
library(tuneRanger)
library(ranger)
library(mlr)
library(OpenML)
model.task = makeClassifTask(data = modelData, target = "Species")
# Estimate runtime
estimateTimeTuneRanger(model.task)
set.seed(153756)
# Tuning
#  res = tuneRanger(model.task, measure = list(multiclass.brier), num.trees = 1000,
#                   num.threads = 2, iters = 70, iters.warmup = 30)
res = tuneRanger(model.task, measure = list(acc), num.trees = 1000,
num.threads = 2, iters = 70, iters.warmup = 30)
res
speciesRF <- ranger(Species ~ ., data = modelData
, num.trees = 1000
, mtry = res$recommended.pars$mtry
, min.node.size = res$recommended.pars$min.node.size
, sample.fraction = c (res$recommended.pars$sample.fraction, res$recommended.pars$sample.fraction)
, importance = "permutation"
)
speciesRF
imp <- importance(speciesRF)
imp <- imp[order(imp)]
imp
# predict using training data
typePred <- predict(speciesRF, data = trainingData[, -1])
table(typePred$predictions, trainingData$Species)
CM <- table(typePred$predictions, trainingData$Species)
confusionMatrix(typePred$predictions, trainingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for original training data: ", accuracy))
# predict using testing data
typePred <- predict(speciesRF, data = testingData[, -1])
table(typePred$predictions, testingData$Species)
CM <- table(typePred$predictions, testingData$Species)
confusionMatrix(typePred$predictions, testingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for testing data: ", accuracy))
typePred <- predict(speciesRF, data = modelData[, -1])
table(typePred$predictions, modelData$Species)
CM <- table(typePred$predictions, modelData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for all data: ", accuracy))
}
imp
################################################################################
# leave-one-out cross validation testing using training/testing data
#
# idea is to build model n times, each time leaving out 1 observation. Predict
# the omitted observation and summarize the prediction error to get test statistic.
################################################################################
library(ranger)
useRanger <- TRUE
folds <- nrow(modelData)
# set seed so we can replicate results
set.seed(153756)
for (iter in c(1:folds)) {
cat(iter, " of ", folds, "\n")
trainingData <- modelData[-iter,]
testingData <- modelData[iter,]
# use hyperparameter values from tuning
if (useRanger) {
# speciesRF <- ranger(Species ~ ., data = trainingData
#                     , mtry = 13
#                     , min.node.size = 5
#                     , sample.fraction = c(0.5476666, 0.5476666)
#                     , num.trees = 1000)
speciesRF <- ranger(Species ~ ., data = trainingData
, mtry = res$recommended.pars$mtry
, min.node.size = res$recommended.pars$min.node.size
, sample.fraction = c (res$recommended.pars$sample.fraction, res$recommended.pars$sample.fraction)
, num.trees = 1000)
#speciesRF
modelOOB <- speciesRF$prediction.error
# predict using testing data
typePred <- predict(speciesRF, data = testingData[, -1])
c <- confusionMatrix(typePred$predictions, testingData$Species)
testAccuracy <- c$overall[[1]]
testKappa <- c$overall[[2]]
typePred <- predict(speciesRF, data = modelData[, -1])
c <- confusionMatrix(typePred$predictions, modelData$Species)
allAccuracy <- c$overall[[1]]
allKappa <- c$overall[[2]]
}
else {
speciesRF <- randomForest(Species ~ ., data = trainingData, mtry = 12, nodesize = 3, ntree = 1000
#                              , sampsize = rep(sum(trainingData$Species == "PSME")
#                                , nlevels(trainingData$Species)
#                                )
)
#speciesRF
modelOOB <- as.double(speciesRF$err.rate[500, 1])
# predict using testing data
typePred <- predict(speciesRF, newdata = testingData[, -1])
c <- confusionMatrix(typePred, testingData$Species)
testAccuracy <- c$overall[[1]]
testKappa <- c$overall[[2]]
typePred <- predict(speciesRF, newdata = modelData[, -1])
c <- confusionMatrix(typePred, modelData$Species)
allAccuracy <- c$overall[[1]]
allKappa <- c$overall[[2]]
}
if (iter == 1) {
accRes <- data.frame(iteration = iter, modelOOB = round(modelOOB * 100, 1), testACC = round(testAccuracy * 100, 1), testKappa = testKappa, allACC = round(allAccuracy * 100, 1), allKappa = allKappa)
} else {
t <- data.frame(iteration = iter, modelOOB = round(modelOOB * 100, 1), testACC = round(testAccuracy * 100, 1), testKappa = testKappa, allACC = round(allAccuracy * 100, 1), allKappa = allKappa)
accRes <- rbind(accRes, t)
}
}
# some of the columns have bad values for the LOOV
t <- accRes %>%
summarize(
AveOOB = mean(modelOOB, na.rm = TRUE),
SDOOB = sd(modelOOB, na.rm = TRUE),
CVOOB = sd(modelOOB, na.rm = TRUE) / mean(modelOOB, na.rm = TRUE),
AveTestACC = mean((100 - testACC), na.rm = TRUE),
SDTestACC = sd((100 - testACC), na.rm = TRUE),
CVTestACC = sd((100 - testACC), na.rm = TRUE) / mean((100 - testACC), na.rm = TRUE),
AveTestKappa = mean(testKappa, na.rm = TRUE),
SDTestKappa = sd(testKappa, na.rm = TRUE),
)
t
#===============================================================================
##### this code needs to have the importance scores from the model with all variables
#===============================================================================
# reorder importance scores from largest to smallest
imp <- imp[order(-imp)]
imp
# drop importance scores of 0.0
imp <- imp[imp > 0]
imp
# build correlation matric for top 30 variables
#a <- modelData[, c(names(imp[1:30]))]
a <- modelData[, c(names(imp))]
c <- abs(cor(a, method = "spearman"))
c[upper.tri(c, diag = TRUE)] <- NA
mc <- apply(c, 1, max, na.rm=TRUE)
v <- mc[mc < 0.50]
v
names(v)
View(c)
library(gridExtra)
# produce boxplots of intensity metrics
library(ggplot2)
ggplot(modelData, aes(x = Species, y = Elev.P99)) + geom_boxplot() + theme(axis.text=element_text(size=12, face="bold"), axis.title=element_text(size=14,face="bold"))
ggplot(modelData, aes(x = Species, y = Elev.P95)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Elev.L3)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Elev.L4)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Elev.minimum)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Int.L.skewness)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Int.skewness)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Int.minimum)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Int.P60)) + geom_boxplot()
ggplot(modelData, aes(x = Species, y = Int.mode)) + geom_boxplot()
