# add names to list using parameters
names(store_maxnode) <- paste("ntrees:", params$ntrees,
"nodesize:", params$nodesize)
# combine results
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
# get best mtry for model
lapply(store_maxnode, function(x) x$best)
# get best average performance for models
t <- lapply(store_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])
# find best model using ROC...should be an easier way to access the list of RF models but
# I couldn't figure it out so I looped.
besti <- 0
bestmtry <- 0
bestnodesize <- 0
bestntree <- 0
bestROC <- 0
for(i in 1:nrow(params)) {
if (t[[i]]$ROC > bestROC) {
besti <- i
bestROC <- t[[i]]$ROC
bestmtry <- t[[i]]$mtry
bestnodesize <- params[i,2]
bestntree <- params[i,1]
}
}
# run best model after looking at results to find model with highest ROC
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = bestntree
, nodesize = bestnodesize
)
print(paste("Best model based on area under ROC:"))
print(paste("   mtry = ", bestmtry))
print(paste("   ntree = ", bestntree))
print(paste("   nodesize = ", bestnodesize))
print(paste("   ROC value = ", bestROC))
} else {
# probst tune
library(tuneRanger)
library(ranger)
library(mlr)
library(OpenML)
model.task = makeClassifTask(data = modelData, target = "Species")
# Estimate runtime
estimateTimeTuneRanger(model.task)
set.seed(153756)
# Tuning
#  res = tuneRanger(model.task, measure = list(multiclass.brier), num.trees = 1000,
#                   num.threads = 2, iters = 70, iters.warmup = 30)
res = tuneRanger(model.task, measure = list(acc), num.trees = 1000,
num.threads = 2, iters = 70, iters.warmup = 30)
res
speciesRF <- ranger(Species ~ ., data = modelData
, num.trees = 1000
, mtry = res$recommended.pars$mtry
, min.node.size = res$recommended.pars$min.node.size
, sample.fraction = c (res$recommended.pars$sample.fraction, res$recommended.pars$sample.fraction)
, importance = "permutation"
)
speciesRF
imp <- importance(speciesRF)
imp <- imp[order(imp)]
imp
# predict using training data
typePred <- predict(speciesRF, data = trainingData[, -1])
table(typePred$predictions, trainingData$Species)
CM <- table(typePred$predictions, trainingData$Species)
confusionMatrix(typePred$predictions, trainingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for original training data: ", accuracy))
# predict using testing data
typePred <- predict(speciesRF, data = testingData[, -1])
table(typePred$predictions, testingData$Species)
CM <- table(typePred$predictions, testingData$Species)
confusionMatrix(typePred$predictions, testingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for testing data: ", accuracy))
typePred <- predict(speciesRF, data = modelData[, -1])
table(typePred$predictions, modelData$Species)
CM <- table(typePred$predictions, modelData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for all data: ", accuracy))
}
confusionMatrix(speciesRF$prediction.error, modelData$Species)
confusionMatrix(speciesRF$predictions, modelData$Species)
modelData <- allData
if (useLeaningTrees) {
# drop rows with bad metrics
modelData <- modelData[modelData$Elev.stddev > 0, ]
}
# drop P95 and P99...testing RF behavior
#modelData <- modelData[, -c(35,36)]
# limit variables
#modelData <- allData[, c("Species", "Elev.P99", "Elev.P95", "Int.L.skewness", "Int.L3")]
# drop the min/max values for elevation because we expect these to be 0 and 3.0 or very close
modelData <- modelData[, -c(2, 3)]
# drop all intensity variables
dropIntensity <- TRUE
dropElevation <- FALSE
if (dropIntensity) {
modelData <- modelData[, c(1:37, 71:85)]
}
if (dropElevation) {
modelData <- modelData[, c(1, 38:70)]
}
# set seed so we can replicate results
set.seed(15356)
# randomly split data into training and testing sets...the split seems to matter. I did
# testing (see end of code) to evaluate the split proportion and the stability of the
# error rates (cross validation with averaged OOB and accuracy). However, I didn't really
# come up with a good strategy for selecting the best split. Intuitively, you want to
# have a model that fits well (low OOB) and also performs well with "new" data (low error
# with testing data). I looked at CV of OOB and testing-data error as possible criteria for
# selecting a final split. My logic is that you want the split that gives roughly the same
# error for both and that produces low CV (indicating model and prediction stability, I think).
#
# The problem is that when you do the cross validation, you are looking at results over
# multiple iterations but you ultimately have to use a single iteration to predict species
# for "new" data so you really don't know anything about the single-iteration model's
# stability.
#
# I think the final strategy is to fit a model using a reasonable split (informed by
# the sensitivity analysis), report the OOB, Kappa, and error for the testing data. Then
# run cross validation using the same split to assess the stability of the split and
# predictor variables. Report the average OOB, Kappa and testing error to indicate that
# our overall data set is providing a classifier that is robust enough for use.
#
# Alternatively, do the cross validation testing and use the results to determine the
# split. I did this and looked for the split that produced the most similar values for
# OOB and error with testing data. 55/35 split was the "best".
#
# Another question is the data used for tuning. Should you use the training data only
# or all data? The tuneRanger() function uses prediction accuracy as the test criteria
# (measure can be changed) but doesn't use a data split. I think it makes the most sense to
# tune parameters using the split data but I haven't been able to find any guidance online.
#
# I did some testing at the end of the code. A 50/50 or 60/40 split seems to produce
# a model with fairly low OOB error and low error when predicting the testing data
train_indx = sample(x = 1:nrow(modelData),
size = 0.55 * nrow(modelData),
replace = FALSE)
trainingData <- modelData[train_indx,]
testingData <- modelData[-train_indx,]
#ind = sample(2, nrow(modelData), replace=FALSE, prob=c(0.8,0.2))
#trainingData = modelData[ind == 1, ]
#testingData = modelData[ind == 2, ]
table(trainingData$Species)
table(testingData$Species)
table(modelData$Species)
if (simpleTune) {
# use a tuning function to find the best mtry value (lowest OOB error)
# the best value will very depending on the random number seed (be careful if you rerun this code without initializing the seed above)
bestmtry <- tuneRF(trainingData[, -1], trainingData$Species, ntreeTry = 1000, stepFactor = 1.5, improve = 0.01, trace=T, plot= T)
i <- which(bestmtry==min(bestmtry), arr.ind = TRUE)
bestmtry <- bestmtry[i[1]]
#bestmtry <- 25
#bestmtry <- 19
print(paste("Best value for mtry = ", bestmtry))
# build a random forest model using the training data... we have more TSHE than PSME so use sampsize
# parameter to equalize the sample. Balanced sample probably isn't needed...species counts aren't that
# different (247 PSME & 278 TSHE). Accuracy is slightly (0.65%) better when NOT balancing the sample.
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = 1000
#                       , nodesize = 4
, maxnodes = bestmtry - 1
)
} else if (probstTune == FALSE) {
# create predefined folds
set.seed(1234)
cv_folds <- createFolds(modelData$Species, k = 5, returnTrain = TRUE)
# create tune control
tuneGrid <- expand.grid(.mtry = c(1 : 15))
# default summary
# ***** need to define metric = "Kappa" in call to train()
# ctrl <- trainControl(method = "cv",
#                      number = 5,
#                      search = 'grid',
#                      classProbs = TRUE,
#                      savePredictions = "final",
#                      index = cv_folds)
ctrl <- trainControl(method = "cv",
number = 5,
search = 'grid',
classProbs = TRUE,
savePredictions = "final",
index = cv_folds,
summaryFunction = twoClassSummary) #in most cases a better summary for two class problems
# add more parameters to tune...also adds run time
ntrees <- c(250, 500, 1000)
nodesize <- c(1, 2, 4, 6)
params <- expand.grid(ntrees = ntrees,
nodesize = nodesize)
# train model
store_maxnode <- vector("list", nrow(params))
for(i in 1:nrow(params)) {
nodesize <- params[i,2]
ntree <- params[i,1]
set.seed(65)
rf_model <- train(Species~.,
data = modelData,
method = "rf",
importance=TRUE,
metric = "ROC",
#                      metric = "Kappa",
tuneGrid = tuneGrid,
trControl = ctrl,
ntree = ntree,
nodesize = nodesize)
store_maxnode[[i]] <- rf_model
}
# add names to list using parameters
names(store_maxnode) <- paste("ntrees:", params$ntrees,
"nodesize:", params$nodesize)
# combine results
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
# get best mtry for model
lapply(store_maxnode, function(x) x$best)
# get best average performance for models
t <- lapply(store_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])
# find best model using ROC...should be an easier way to access the list of RF models but
# I couldn't figure it out so I looped.
besti <- 0
bestmtry <- 0
bestnodesize <- 0
bestntree <- 0
bestROC <- 0
for(i in 1:nrow(params)) {
if (t[[i]]$ROC > bestROC) {
besti <- i
bestROC <- t[[i]]$ROC
bestmtry <- t[[i]]$mtry
bestnodesize <- params[i,2]
bestntree <- params[i,1]
}
}
# run best model after looking at results to find model with highest ROC
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = bestntree
, nodesize = bestnodesize
)
print(paste("Best model based on area under ROC:"))
print(paste("   mtry = ", bestmtry))
print(paste("   ntree = ", bestntree))
print(paste("   nodesize = ", bestnodesize))
print(paste("   ROC value = ", bestROC))
} else {
# probst tune
library(tuneRanger)
library(ranger)
library(mlr)
library(OpenML)
model.task = makeClassifTask(data = modelData, target = "Species")
# Estimate runtime
estimateTimeTuneRanger(model.task)
set.seed(153756)
# Tuning
#  res = tuneRanger(model.task, measure = list(multiclass.brier), num.trees = 1000,
#                   num.threads = 2, iters = 70, iters.warmup = 30)
res = tuneRanger(model.task, measure = list(acc), num.trees = 1000,
num.threads = 2, iters = 70, iters.warmup = 30)
res
speciesRF <- ranger(Species ~ ., data = modelData
, num.trees = 1000
, mtry = res$recommended.pars$mtry
, min.node.size = res$recommended.pars$min.node.size
, sample.fraction = c (res$recommended.pars$sample.fraction, res$recommended.pars$sample.fraction)
, importance = "permutation"
)
speciesRF
imp <- importance(speciesRF)
imp <- imp[order(imp)]
imp
# predict using training data
typePred <- predict(speciesRF, data = trainingData[, -1])
table(typePred$predictions, trainingData$Species)
CM <- table(typePred$predictions, trainingData$Species)
confusionMatrix(typePred$predictions, trainingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for original training data: ", accuracy))
# predict using testing data
typePred <- predict(speciesRF, data = testingData[, -1])
table(typePred$predictions, testingData$Species)
CM <- table(typePred$predictions, testingData$Species)
confusionMatrix(typePred$predictions, testingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for testing data: ", accuracy))
typePred <- predict(speciesRF, data = modelData[, -1])
table(typePred$predictions, modelData$Species)
CM <- table(typePred$predictions, modelData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for all data: ", accuracy))
}
confusionMatrix(speciesRF$predictions, modelData$Species)
modelData <- allData
if (useLeaningTrees) {
# drop rows with bad metrics
modelData <- modelData[modelData$Elev.stddev > 0, ]
}
# drop P95 and P99...testing RF behavior
#modelData <- modelData[, -c(35,36)]
# limit variables
#modelData <- allData[, c("Species", "Elev.P99", "Elev.P95", "Int.L.skewness", "Int.L3")]
# drop the min/max values for elevation because we expect these to be 0 and 3.0 or very close
modelData <- modelData[, -c(2, 3)]
# drop all intensity variables
dropIntensity <- FALSE
dropElevation <- TRUE
if (dropIntensity) {
modelData <- modelData[, c(1:37, 71:85)]
}
if (dropElevation) {
modelData <- modelData[, c(1, 38:70)]
}
# set seed so we can replicate results
set.seed(15356)
train_indx = sample(x = 1:nrow(modelData),
size = 0.55 * nrow(modelData),
replace = FALSE)
trainingData <- modelData[train_indx,]
testingData <- modelData[-train_indx,]
#ind = sample(2, nrow(modelData), replace=FALSE, prob=c(0.8,0.2))
#trainingData = modelData[ind == 1, ]
#testingData = modelData[ind == 2, ]
table(trainingData$Species)
table(testingData$Species)
table(modelData$Species)
modelData %>%
group_by(Species) %>%
summarize(
Count = n(),
AveP99 = mean(Elev.P99, na.rm = TRUE),
IntP80 = mean(Int.P80, na.rm = TRUE),
IntP20 = mean(Int.P20, na.rm = TRUE),
IntIQ = mean(Int.IQ, na.rm = TRUE)
)
if (simpleTune) {
# use a tuning function to find the best mtry value (lowest OOB error)
# the best value will very depending on the random number seed (be careful if you rerun this code without initializing the seed above)
bestmtry <- tuneRF(trainingData[, -1], trainingData$Species, ntreeTry = 1000, stepFactor = 1.5, improve = 0.01, trace=T, plot= T)
i <- which(bestmtry==min(bestmtry), arr.ind = TRUE)
bestmtry <- bestmtry[i[1]]
#bestmtry <- 25
#bestmtry <- 19
print(paste("Best value for mtry = ", bestmtry))
# build a random forest model using the training data... we have more TSHE than PSME so use sampsize
# parameter to equalize the sample. Balanced sample probably isn't needed...species counts aren't that
# different (247 PSME & 278 TSHE). Accuracy is slightly (0.65%) better when NOT balancing the sample.
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = 1000
#                       , nodesize = 4
, maxnodes = bestmtry - 1
)
} else if (probstTune == FALSE) {
# create predefined folds
set.seed(1234)
cv_folds <- createFolds(modelData$Species, k = 5, returnTrain = TRUE)
# create tune control
tuneGrid <- expand.grid(.mtry = c(1 : 15))
# default summary
# ***** need to define metric = "Kappa" in call to train()
# ctrl <- trainControl(method = "cv",
#                      number = 5,
#                      search = 'grid',
#                      classProbs = TRUE,
#                      savePredictions = "final",
#                      index = cv_folds)
ctrl <- trainControl(method = "cv",
number = 5,
search = 'grid',
classProbs = TRUE,
savePredictions = "final",
index = cv_folds,
summaryFunction = twoClassSummary) #in most cases a better summary for two class problems
# add more parameters to tune...also adds run time
ntrees <- c(250, 500, 1000)
nodesize <- c(1, 2, 4, 6)
params <- expand.grid(ntrees = ntrees,
nodesize = nodesize)
# train model
store_maxnode <- vector("list", nrow(params))
for(i in 1:nrow(params)) {
nodesize <- params[i,2]
ntree <- params[i,1]
set.seed(65)
rf_model <- train(Species~.,
data = modelData,
method = "rf",
importance=TRUE,
metric = "ROC",
#                      metric = "Kappa",
tuneGrid = tuneGrid,
trControl = ctrl,
ntree = ntree,
nodesize = nodesize)
store_maxnode[[i]] <- rf_model
}
# add names to list using parameters
names(store_maxnode) <- paste("ntrees:", params$ntrees,
"nodesize:", params$nodesize)
# combine results
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
# get best mtry for model
lapply(store_maxnode, function(x) x$best)
# get best average performance for models
t <- lapply(store_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])
# find best model using ROC...should be an easier way to access the list of RF models but
# I couldn't figure it out so I looped.
besti <- 0
bestmtry <- 0
bestnodesize <- 0
bestntree <- 0
bestROC <- 0
for(i in 1:nrow(params)) {
if (t[[i]]$ROC > bestROC) {
besti <- i
bestROC <- t[[i]]$ROC
bestmtry <- t[[i]]$mtry
bestnodesize <- params[i,2]
bestntree <- params[i,1]
}
}
# run best model after looking at results to find model with highest ROC
speciesRF <- randomForest(Species ~ .
, data = trainingData
#                       , importance = TRUE
#                       , sampsize = rep(sum(trainingData$Species == "PSME")
#                                        , nlevels(trainingData$Species)
#                                       )
, mtry = bestmtry
, ntree = bestntree
, nodesize = bestnodesize
)
print(paste("Best model based on area under ROC:"))
print(paste("   mtry = ", bestmtry))
print(paste("   ntree = ", bestntree))
print(paste("   nodesize = ", bestnodesize))
print(paste("   ROC value = ", bestROC))
} else {
# probst tune
library(tuneRanger)
library(ranger)
library(mlr)
library(OpenML)
model.task = makeClassifTask(data = modelData, target = "Species")
# Estimate runtime
estimateTimeTuneRanger(model.task)
set.seed(153756)
# Tuning
#  res = tuneRanger(model.task, measure = list(multiclass.brier), num.trees = 1000,
#                   num.threads = 2, iters = 70, iters.warmup = 30)
res = tuneRanger(model.task, measure = list(acc), num.trees = 1000,
num.threads = 2, iters = 70, iters.warmup = 30)
res
speciesRF <- ranger(Species ~ ., data = modelData
, num.trees = 1000
, mtry = res$recommended.pars$mtry
, min.node.size = res$recommended.pars$min.node.size
, sample.fraction = c (res$recommended.pars$sample.fraction, res$recommended.pars$sample.fraction)
, importance = "permutation"
)
speciesRF
imp <- importance(speciesRF)
imp <- imp[order(imp)]
imp
# predict using training data
typePred <- predict(speciesRF, data = trainingData[, -1])
table(typePred$predictions, trainingData$Species)
CM <- table(typePred$predictions, trainingData$Species)
confusionMatrix(typePred$predictions, trainingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for original training data: ", accuracy))
# predict using testing data
typePred <- predict(speciesRF, data = testingData[, -1])
table(typePred$predictions, testingData$Species)
CM <- table(typePred$predictions, testingData$Species)
confusionMatrix(typePred$predictions, testingData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for testing data: ", accuracy))
typePred <- predict(speciesRF, data = modelData[, -1])
table(typePred$predictions, modelData$Species)
CM <- table(typePred$predictions, modelData$Species)
accuracy <- (sum(diag(CM)))/sum(CM)
print(paste("Accuracy for all data: ", accuracy))
}
confusionMatrix(speciesRF$predictions, modelData$Species)
names(v)
